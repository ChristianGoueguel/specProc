% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/yGradient_glsw.R
\name{yGradient_glsw}
\alias{yGradient_glsw}
\title{Y-Gradient Generalized Least Squares Weighting}
\usage{
yGradient_glsw(X, Y, alpha = 0.01)
}
\arguments{
\item{X}{A numeric matrix representing the input data.}

\item{Y}{A numeric vector representing the response vector.}

\item{alpha}{A numeric value specifying the weighting parameter. Typical values
range from 1 to 0.0001. Default is 0.01.}
}
\value{
A tibble containing the filtering matrix.
}
\description{
This function implements the Generalized Least Squares Weighting (GLSW) algorithm
with a specific approach for filtering out X (input data) variance orthogonal to
a Y (response vector). The function calculates the difference matrix X_diff by
first sorting the rows of X and Y in order of increasing Y value, and then
calculating the first-order Savitzky-Golay derivative of each column of X and Y.
The rows of X_diff are then re-weighted based on the differences in Y values.
}
\details{
The filtering matrix G can be used to down-weight correlations in the original
covariance matrix, effectively reducing the impact of interferences on the data.
}
\references{
\itemize{
\item Geladi, P., & Kowalski, B. R. (1986). Partial least-squares regression: a tutorial.
Analytica Chimica Acta, 185, 1-17.
\item Martens, H., & Naes, T. (1989). Multivariate calibration. John Wiley & Sons.
\item Wold, S., Ruhe, A., Wold, H., & Dunn III, W. J. (1984). The collinearity problem in linear regression.
The partial least squares (PLS) approach to generalized inverses. SIAM Journal on Scientific and
Statistical Computing, 5(3), 735-743.
\item Geladi, P., & Kowalski, B. R. (1986). An example of two-block predictive partial least-squares
regression with simulated data. Analytica Chimica Acta, 185, 19-32.
\item Eigenvector Research: Advanced Preprocessing: Multivariate Filtering.
https://wiki.eigenvector.com/index.php?title=Advanced_Preprocessing:_Multivariate_Filtering
}
}
